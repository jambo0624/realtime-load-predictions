{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time Load Forecasting and Resource Management - Advanced Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    tf_available = True\n",
    "except ImportError:\n",
    "    tf_available = False\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extended feature set data\n",
    "data_path = '../processed_data/c7_user_DrrEIEW_timeseries.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Successfully loaded data, shape: {df.shape}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(df.head())\n",
    "    \n",
    "    # View data types and basic information\n",
    "    display(df.info())\n",
    "    \n",
    "    # View statistical summary of numerical features\n",
    "    display(df.describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percentage\n",
    "}).sort_values('Missing Percentage', ascending=False)\n",
    "\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Time feature inspection and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and transform time features\n",
    "time_columns = [col for col in df.columns if 'time' in col.lower() and 'dt' not in col.lower()]\n",
    "print(f\"Time-related columns: {time_columns}\")\n",
    "\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n",
    "            df[f'{col}_dt'] = pd.to_datetime(df[col], unit='us')\n",
    "            print(f\"Converting column {col} to datetime format\")\n",
    "\n",
    "# Ensure time series index\n",
    "if 'time_dt' in df.columns:\n",
    "    # Set time column as index\n",
    "    df_ts = df.set_index('time_dt').sort_index()\n",
    "    print(\"Time column set as index and sorted\")\n",
    "    display(df_ts.head())\n",
    "else:\n",
    "    print(\"time_dt column not found, checking time column conversion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 The distribution of the target variable and visualization of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variables (CPU and memory usage)\n",
    "target_vars = ['average_usage_cpu', 'average_usage_memory']\n",
    "\n",
    "# Check if target variables exist\n",
    "target_vars = [var for var in target_vars if var in df.columns]\n",
    "\n",
    "if target_vars:\n",
    "    # Visualize target variable distributions\n",
    "    fig, axes = plt.subplots(len(target_vars), 1, figsize=(12, 5*len(target_vars)))\n",
    "    if len(target_vars) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i, var in enumerate(target_vars):\n",
    "        # Histogram\n",
    "        sns.histplot(df[var], ax=axes[i], kde=True)\n",
    "        axes[i].set_title(f'{var} Distribution')\n",
    "        axes[i].set_xlabel(var)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Time series visualization\n",
    "    if 'time_dt' in df.columns:\n",
    "        fig, axes = plt.subplots(len(target_vars), 1, figsize=(16, 6*len(target_vars)))\n",
    "        if len(target_vars) == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for i, var in enumerate(target_vars):\n",
    "            axes[i].plot(df['time_dt'], df[var])\n",
    "            axes[i].set_title(f'{var} Time Series')\n",
    "            axes[i].set_xlabel('Time')\n",
    "            axes[i].set_ylabel(var)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Target variables not found in dataset, please check column names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "Based on the previous model results and data analysis, we will create more advanced features to enhance the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creation of temporal characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time features\n",
    "def create_time_features(df, time_col='time_dt'):\n",
    "    \"\"\"Create rich time features from time column\"\"\"\n",
    "    print(\"\\nCreating time features...\")\n",
    "    \n",
    "    # Ensure column exists\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Column {time_col} does not exist\")\n",
    "        return df\n",
    "    \n",
    "    # Copy dataframe to avoid modifying original data\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Ensure time column is datetime type\n",
    "    df_new[time_col] = pd.to_datetime(df_new[time_col])\n",
    "    print(f\"Converting {time_col} to datetime type\")\n",
    "    \n",
    "    # Create features from datetime\n",
    "    df_new['hour_of_day'] = df_new[time_col].dt.hour\n",
    "    df_new['day_of_week'] = df_new[time_col].dt.dayofweek\n",
    "    df_new['day_of_month'] = df_new[time_col].dt.day\n",
    "    df_new['month'] = df_new[time_col].dt.month\n",
    "    \n",
    "    # Create weekend indicator (0=weekday, 1=weekend)\n",
    "    df_new['is_weekend'] = df_new['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # Create time of day classification\n",
    "    def get_day_part(hour):\n",
    "        if 5 <= hour < 12:\n",
    "            return 'morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'afternoon'\n",
    "        elif 17 <= hour < 22:\n",
    "            return 'evening'\n",
    "        else:\n",
    "            return 'night'\n",
    "    \n",
    "    df_new['day_part'] = df_new['hour_of_day'].apply(get_day_part)\n",
    "    \n",
    "    # One-hot encode time periods\n",
    "    df_new = pd.get_dummies(df_new, columns=['day_part'], prefix='day_part')\n",
    "    \n",
    "    # Create cyclical features for hour and date (sine and cosine transformations)\n",
    "    df_new['hour_sin'] = np.sin(2 * np.pi * df_new['hour_of_day'] / 24)\n",
    "    df_new['hour_cos'] = np.cos(2 * np.pi * df_new['hour_of_day'] / 24)\n",
    "    df_new['day_sin'] = np.sin(2 * np.pi * df_new['day_of_week'] / 7)\n",
    "    df_new['day_cos'] = np.cos(2 * np.pi * df_new['day_of_week'] / 7)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply time feature creation function\n",
    "if 'time_dt' in df.columns:\n",
    "    df = create_time_features(df)\n",
    "    print(\"Time features created\")\n",
    "    \n",
    "    # Display newly created time feature columns\n",
    "    time_feature_cols = ['hour_of_day', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos']\n",
    "    time_feature_cols = [col for col in time_feature_cols if col in df.columns]\n",
    "    \n",
    "    if time_feature_cols:\n",
    "        display(df[time_feature_cols].head())\n",
    "else:\n",
    "    print(\"Cannot create time features, missing time_dt column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Sliding window feature (lag feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (based on sorted time series)\n",
    "def create_lag_features(df, target_cols, lag_periods=[1, 3, 6, 12, 24], sort_col='time_dt'):\n",
    "    \"\"\"Create lag features for target columns\"\"\"\n",
    "    # Ensure data is sorted by time\n",
    "    df_sorted = df.sort_values(by=sort_col).copy()\n",
    "    \n",
    "    # Create features for each target column and lag period\n",
    "    for target in target_cols:\n",
    "        for lag in lag_periods:\n",
    "            # Create lag feature\n",
    "            df_sorted[f'{target}_lag_{lag}'] = df_sorted[target].shift(lag)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "# Create rolling window statistical features\n",
    "def create_rolling_features(df, target_cols, windows=[3, 6, 12, 24], sort_col='time_dt'):\n",
    "    \"\"\"Create rolling window statistical features for target columns\"\"\"\n",
    "    # Ensure data is sorted by time\n",
    "    df_sorted = df.sort_values(by=sort_col).copy()\n",
    "    \n",
    "    # Create features for each target column and window\n",
    "    for target in target_cols:\n",
    "        for window in windows:\n",
    "            # Create rolling mean\n",
    "            df_sorted[f'{target}_rolling_mean_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).mean()\n",
    "            # Create rolling standard deviation\n",
    "            df_sorted[f'{target}_rolling_std_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).std()\n",
    "            # Create rolling min and max\n",
    "            df_sorted[f'{target}_rolling_min_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).min()\n",
    "            df_sorted[f'{target}_rolling_max_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).max()\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "# Apply lag and rolling window feature creation\n",
    "if 'time_dt' in df.columns and target_vars:\n",
    "    # Create lag features\n",
    "    df = create_lag_features(df, target_vars)\n",
    "    print(\"Lag features created\")\n",
    "    \n",
    "    # Create rolling window features\n",
    "    df = create_rolling_features(df, target_vars)\n",
    "    print(\"Rolling window features created\")\n",
    "    \n",
    "    # Display first few rows of new features\n",
    "    lag_cols = [col for col in df.columns if 'lag_' in col or 'rolling_' in col][:5]\n",
    "    if lag_cols:\n",
    "        display(df[lag_cols].head(10))\n",
    "else:\n",
    "    print(\"Cannot create time series features, missing required columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare model training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df, target_vars):\n",
    "    \"\"\"Prepare data for model training\"\"\"\n",
    "    print(\"\\nPreparing data for model training...\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(\"Handling missing values...\")\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                # Fill numeric columns with median\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                # Fill non-numeric columns with mode\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Remove unnecessary columns\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    # Remove high-cardinality ID columns\n",
    "    id_cols = [col for col in df.columns if 'id' in col.lower() or 'name' in col.lower() or 'user' in col.lower()]\n",
    "    cols_to_drop.extend(id_cols)\n",
    "    \n",
    "    # Remove original timestamp columns (keep converted dt columns)\n",
    "    timestamp_cols = [col for col in df.columns if ('time' in col.lower() and 'dt' not in col.lower())]\n",
    "    cols_to_drop.extend(timestamp_cols)\n",
    "    \n",
    "    # Exclude target variables\n",
    "    cols_to_drop = [col for col in cols_to_drop if col not in target_vars]\n",
    "    \n",
    "    # Remove columns that are all NaN\n",
    "    null_cols = df.columns[df.isnull().all()].tolist()\n",
    "    cols_to_drop.extend(null_cols)\n",
    "    \n",
    "    # Drop columns\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
    "    print(f\"Dropped {len(cols_to_drop)} columns\")\n",
    "    \n",
    "    # Convert categorical variables to numeric\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in object_cols:\n",
    "        if col not in target_vars:  # Don't transform target variables\n",
    "            # Label encode categorical variables\n",
    "            df[col] = pd.factorize(df[col])[0]\n",
    "    \n",
    "    print(\"Data preparation completed\")\n",
    "    return df\n",
    "\n",
    "df = prepare_data_for_modeling(df, target_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build a model for each target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions_separate(y_true, predictions_dict, title_prefix=\"Prediction Comparison\"):\n",
    "    \"\"\"Create a separate prediction vs true value comparison chart for each model\"\"\"\n",
    "    \n",
    "    # First, create an overview chart containing all models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(y_true)), y_true, 'k-', label='True Value')\n",
    "    \n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        plt.plot(range(len(preds)), preds, '--', label=f'{model_name} Prediction')\n",
    "    \n",
    "    plt.title(f\"{title_prefix} - Overview\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Target Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title_prefix.replace(' ', '_')}_overview.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Then, create a separate chart for each model\n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(len(y_true)), y_true, 'k-', label='True Value')\n",
    "        plt.plot(range(len(preds)), preds, 'r--', label=f'{model_name} Prediction')\n",
    "        \n",
    "        plt.title(f\"{title_prefix} - {model_name}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Target Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{title_prefix.replace(' ', '_')}_{model_name.replace(' ', '_')}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"Created {len(predictions_dict) + 1} prediction comparison charts\")\n",
    "\n",
    "def evaluate_models(df, target_var, test_size=0.2, random_state=42):\n",
    "    \"\"\"Build and evaluate multiple prediction models\"\"\"\n",
    "    print(f\"\\nEvaluating prediction models for {target_var}...\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    y = df[target_var]\n",
    "    X = df.drop(columns=[col for col in df.columns if col in [target_var] or col.startswith('time_')])\n",
    "    \n",
    "    print(f\"Feature count: {X.shape[1]}\")\n",
    "    print(f\"Sample count: {X.shape[0]}\")\n",
    "    \n",
    "    # Create training and test sets (time series split)\n",
    "    # To ensure we don't use future data to predict the past, use the last test_size proportion as test set\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Feature standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Model results storage\n",
    "    model_results = []\n",
    "    \n",
    "    # 1. ARIMA model\n",
    "    try:\n",
    "        print(\"\\nTraining ARIMA model...\")\n",
    "        # Simplified ARIMA, using only target variable time series\n",
    "        model = ARIMA(y_train, order=(5,1,0))\n",
    "        arima_model = model.fit()\n",
    "        \n",
    "        # Predict\n",
    "        arima_preds = arima_model.forecast(steps=len(y_test))\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, arima_preds))\n",
    "        mae = mean_absolute_error(y_test, arima_preds)\n",
    "        r2 = r2_score(y_test, arima_preds)\n",
    "        \n",
    "        print(f\"ARIMA - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "        model_results.append({\"model\": \"ARIMA\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA model training failed: {e}\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"\\nTraining Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    rf_preds = rf.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, rf_preds))\n",
    "    mae = mean_absolute_error(y_test, rf_preds)\n",
    "    r2 = r2_score(y_test, rf_preds)\n",
    "    \n",
    "    print(f\"Random Forest - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "    model_results.append({\"model\": \"Random Forest\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 feature importance:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # 3. XGBoost\n",
    "    print(\"\\nTraining XGBoost...\")\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=random_state)\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    xgb_preds = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))\n",
    "    mae = mean_absolute_error(y_test, xgb_preds)\n",
    "    r2 = r2_score(y_test, xgb_preds)\n",
    "    \n",
    "    print(f\"XGBoost - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "    model_results.append({\"model\": \"XGBoost\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    \n",
    "    # 4. LSTM \n",
    "    if tf_available and len(X_train) > 50:  # Ensure there's enough data\n",
    "        try:\n",
    "            print(\"\\nTraining LSTM model...\")\n",
    "            \n",
    "            # Prepare LSTM input (samples, timesteps, features)\n",
    "            LOOKBACK = 5\n",
    "            \n",
    "            def create_sequences(X, y, time_steps=LOOKBACK):\n",
    "                X_seq, y_seq = [], []\n",
    "                for i in range(len(X) - time_steps):\n",
    "                    X_seq.append(X[i:i + time_steps])\n",
    "                    y_seq.append(y[i + time_steps])\n",
    "                return np.array(X_seq), np.array(y_seq)\n",
    "            \n",
    "            # Create sequences\n",
    "            X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values)\n",
    "            \n",
    "            # Build LSTM model\n",
    "            model = Sequential([\n",
    "                LSTM(50, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), return_sequences=True),\n",
    "                Dropout(0.2),\n",
    "                LSTM(50, activation='relu'),\n",
    "                Dropout(0.2),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            \n",
    "            # Train\n",
    "            history = model.fit(\n",
    "                X_train_seq, y_train_seq,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Create sequences for test set\n",
    "            X_full = np.vstack((X_train_scaled[-LOOKBACK:], X_test_scaled))\n",
    "            X_test_seq = []\n",
    "            for i in range(len(X_test)):\n",
    "                X_test_seq.append(X_full[i:i + LOOKBACK])\n",
    "            X_test_seq = np.array(X_test_seq)\n",
    "            \n",
    "            # Predict\n",
    "            lstm_preds = model.predict(X_test_seq).flatten()\n",
    "            \n",
    "            # Evaluate\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, lstm_preds))\n",
    "            mae = mean_absolute_error(y_test, lstm_preds)\n",
    "            r2 = r2_score(y_test, lstm_preds)\n",
    "            \n",
    "            print(f\"LSTM - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "            model_results.append({\"model\": \"LSTM\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"LSTM model training failed: {e}\")\n",
    "    \n",
    "    # Summarize results\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df = results_df.sort_values('rmse')\n",
    "    \n",
    "    print(\"\\nModel performance summary:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 可视化预测结果\n",
    "    visualize_predictions_separate(\n",
    "        y_true=y_test, \n",
    "        predictions_dict=predictions_dict,\n",
    "        title_prefix=f\"{target_var} Prediction Comparison\"\n",
    "    )\n",
    "    \n",
    "    # Return the best model and evaluation results\n",
    "    return results_df, feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_var in target_vars:\n",
    "    # Filter out rows with NaN values\n",
    "    df_clean = df.dropna(subset=[target_var])\n",
    "    \n",
    "    # Filter out lag features of other target variables\n",
    "    other_targets = [t for t in target_vars if t != target_var]\n",
    "    cols_to_drop = []\n",
    "    for other_target in other_targets:\n",
    "        cols_to_drop.extend([col for col in df_clean.columns if col.startswith(f\"{other_target}_lag_\")])\n",
    "        cols_to_drop.extend([col for col in df_clean.columns if col.startswith(f\"{other_target}_rolling_\")])\n",
    "    \n",
    "    df_model = df_clean.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # Build and evaluate models\n",
    "    results_df, feature_importance = evaluate_models(df_model, target_var)\n",
    "    \n",
    "    # Save results\n",
    "    results_df.to_csv(f\"model_results_{target_var}.csv\", index=False)\n",
    "    feature_importance.to_csv(f\"feature_importance_{target_var}.csv\", index=False)\n",
    "\n",
    "print(\"Modeling completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
