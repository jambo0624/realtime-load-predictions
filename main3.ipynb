{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实时负载预测与资源管理 - 高级建模 (main3)\n",
    "\n",
    "本笔记本将使用扩展特征集实现更先进的预测模型。基于处理过的数据，我们将探索更复杂的特征工程和模型架构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 机器学习库\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# 深度学习库\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    tf_available = True\n",
    "except ImportError:\n",
    "    print(\"警告: TensorFlow不可用，LSTM模型将不可用\")\n",
    "    tf_available = False\n",
    "\n",
    "# 设置可视化样式\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# 设置pandas显示选项\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载与探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载扩展特征集的数据\n",
    "data_path = 'processed_all_fields_data/c7_user_DrrEIEW_timeseries.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"成功读取数据，形状: {df.shape}\")\n",
    "    \n",
    "    # 显示前几行数据\n",
    "    display(df.head())\n",
    "    \n",
    "    # 查看数据类型和基本信息\n",
    "    display(df.info())\n",
    "    \n",
    "    # 查看数值特征的统计摘要\n",
    "    display(df.describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"读取数据时出错: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 检查缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查缺失值\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    '缺失值数量': missing_values,\n",
    "    '缺失百分比': missing_percentage\n",
    "}).sort_values('缺失百分比', ascending=False)\n",
    "\n",
    "display(missing_df[missing_df['缺失值数量'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 时间特征检查与转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查并转换时间特征\n",
    "time_columns = [col for col in df.columns if 'time' in col.lower() and 'dt' not in col.lower()]\n",
    "print(f\"时间相关列: {time_columns}\")\n",
    "\n",
    "for col in time_columns:\n",
    "    if col in df.columns:\n",
    "        if df[col].dtype == 'int64' or df[col].dtype == 'float64':\n",
    "            df[f'{col}_dt'] = pd.to_datetime(df[col], unit='us')\n",
    "            print(f\"转换列 {col} 为日期时间格式\")\n",
    "\n",
    "# 确保有时间序列索引\n",
    "if 'time_dt' in df.columns:\n",
    "    # 将时间列设为索引\n",
    "    df_ts = df.set_index('time_dt').sort_index()\n",
    "    print(\"已将时间列设为索引并排序\")\n",
    "    display(df_ts.head())\n",
    "else:\n",
    "    print(\"没有找到time_dt列，检查时间列转换\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 目标变量的分布与时间序列可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义目标变量 (CPU和内存使用率)\n",
    "target_vars = ['average_usage_cpu', 'average_usage_memory']\n",
    "\n",
    "# 检查目标变量是否存在\n",
    "target_vars = [var for var in target_vars if var in df.columns]\n",
    "\n",
    "if target_vars:\n",
    "    # 可视化目标变量分布\n",
    "    fig, axes = plt.subplots(len(target_vars), 1, figsize=(12, 5*len(target_vars)))\n",
    "    if len(target_vars) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for i, var in enumerate(target_vars):\n",
    "        # 直方图\n",
    "        sns.histplot(df[var], ax=axes[i], kde=True)\n",
    "        axes[i].set_title(f'{var} Distribution')\n",
    "        axes[i].set_xlabel(var)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 时间序列可视化\n",
    "    if 'time_dt' in df.columns:\n",
    "        fig, axes = plt.subplots(len(target_vars), 1, figsize=(16, 6*len(target_vars)))\n",
    "        if len(target_vars) == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for i, var in enumerate(target_vars):\n",
    "            axes[i].plot(df['time_dt'], df[var])\n",
    "            axes[i].set_title(f'{var} Time Series')\n",
    "            axes[i].set_xlabel('Time')\n",
    "            axes[i].set_ylabel(var)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"目标变量不存在于数据集中，请检查列名\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 特征工程\n",
    "\n",
    "基于之前的模型结果和数据分析，我们将创建更多高级特征来提升模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 时间特征创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建时间特征\n",
    "def create_time_features(df, time_col='time_dt'):\n",
    "    \"\"\"从时间列创建丰富的时间特征\"\"\"\n",
    "    print(\"\\n创建时间特征...\")\n",
    "    \n",
    "    # 确保列存在\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"列 {time_col} 不存在\")\n",
    "        return df\n",
    "    \n",
    "    # 复制数据框以避免修改原始数据\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # 确保时间列是datetime类型\n",
    "    df_new[time_col] = pd.to_datetime(df_new[time_col])\n",
    "    print(f\"转换 {time_col} 为datetime类型\")\n",
    "    \n",
    "    # 从datetime创建特征\n",
    "    df_new['hour_of_day'] = df_new[time_col].dt.hour\n",
    "    df_new['day_of_week'] = df_new[time_col].dt.dayofweek\n",
    "    df_new['day_of_month'] = df_new[time_col].dt.day\n",
    "    df_new['month'] = df_new[time_col].dt.month\n",
    "    \n",
    "    # 创建周末指标 (0=工作日, 1=周末)\n",
    "    df_new['is_weekend'] = df_new['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # 创建一天中的时段分类\n",
    "    def get_day_part(hour):\n",
    "        if 5 <= hour < 12:\n",
    "            return 'morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'afternoon'\n",
    "        elif 17 <= hour < 22:\n",
    "            return 'evening'\n",
    "        else:\n",
    "            return 'night'\n",
    "    \n",
    "    df_new['day_part'] = df_new['hour_of_day'].apply(get_day_part)\n",
    "    \n",
    "    # 对时段进行独热编码\n",
    "    df_new = pd.get_dummies(df_new, columns=['day_part'], prefix='day_part')\n",
    "    \n",
    "    # 创建小时和日期的周期性特征（正弦和余弦变换）\n",
    "    df_new['hour_sin'] = np.sin(2 * np.pi * df_new['hour_of_day'] / 24)\n",
    "    df_new['hour_cos'] = np.cos(2 * np.pi * df_new['hour_of_day'] / 24)\n",
    "    df_new['day_sin'] = np.sin(2 * np.pi * df_new['day_of_week'] / 7)\n",
    "    df_new['day_cos'] = np.cos(2 * np.pi * df_new['day_of_week'] / 7)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# 应用时间特征创建函数\n",
    "if 'time_dt' in df.columns:\n",
    "    df = create_time_features(df)\n",
    "    print(\"已创建时间特征\")\n",
    "    \n",
    "    # 显示新增的时间特征列\n",
    "    time_feature_cols = ['hour_of_day', 'day_of_week', 'is_weekend', 'hour_sin', 'hour_cos']\n",
    "    time_feature_cols = [col for col in time_feature_cols if col in df.columns]\n",
    "    \n",
    "    if time_feature_cols:\n",
    "        display(df[time_feature_cols].head())\n",
    "else:\n",
    "    print(\"无法创建时间特征，缺少time_dt列\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 滑动窗口特征（滞后特征）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建滞后特征（基于排序后的时间序列）\n",
    "def create_lag_features(df, target_cols, lag_periods=[1, 3, 6, 12, 24], sort_col='time_dt'):\n",
    "    \"\"\"为目标列创建滞后特征\"\"\"\n",
    "    # 确保数据按时间排序\n",
    "    df_sorted = df.sort_values(by=sort_col).copy()\n",
    "    \n",
    "    # 为每个目标列和每个滞后周期创建特征\n",
    "    for target in target_cols:\n",
    "        for lag in lag_periods:\n",
    "            # 创建滞后特征\n",
    "            df_sorted[f'{target}_lag_{lag}'] = df_sorted[target].shift(lag)\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "# 创建滚动窗口统计特征\n",
    "def create_rolling_features(df, target_cols, windows=[3, 6, 12, 24], sort_col='time_dt'):\n",
    "    \"\"\"为目标列创建滚动窗口统计特征\"\"\"\n",
    "    # 确保数据按时间排序\n",
    "    df_sorted = df.sort_values(by=sort_col).copy()\n",
    "    \n",
    "    # 为每个目标列和每个窗口创建特征\n",
    "    for target in target_cols:\n",
    "        for window in windows:\n",
    "            # 创建滚动平均值\n",
    "            df_sorted[f'{target}_rolling_mean_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).mean()\n",
    "            # 创建滚动标准差\n",
    "            df_sorted[f'{target}_rolling_std_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).std()\n",
    "            # 创建滚动最小值和最大值\n",
    "            df_sorted[f'{target}_rolling_min_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).min()\n",
    "            df_sorted[f'{target}_rolling_max_{window}'] = df_sorted[target].rolling(window=window, min_periods=1).max()\n",
    "    \n",
    "    return df_sorted\n",
    "\n",
    "# 应用滞后和滚动窗口特征创建\n",
    "if 'time_dt' in df.columns and target_vars:\n",
    "    # 创建滞后特征\n",
    "    df = create_lag_features(df, target_vars)\n",
    "    print(\"已创建滞后特征\")\n",
    "    \n",
    "    # 创建滚动窗口特征\n",
    "    df = create_rolling_features(df, target_vars)\n",
    "    print(\"已创建滚动窗口特征\")\n",
    "    \n",
    "    # 显示新增特征的前几行\n",
    "    lag_cols = [col for col in df.columns if 'lag_' in col or 'rolling_' in col][:5]\n",
    "    if lag_cols:\n",
    "        display(df[lag_cols].head(10))\n",
    "else:\n",
    "    print(\"无法创建时间序列特征，缺少必要的列\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 资源使用率特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建资源使用率特征\n",
    "def create_utilization_features(df):\n",
    "    \"\"\"创建资源使用率特征\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # 检查必要的列是否存在\n",
    "    if 'resource_request_cpu' in df.columns and 'average_usage_cpu' in df.columns:\n",
    "        # CPU使用率 = 实际使用 / 请求资源\n",
    "        df_new['cpu_utilization_ratio'] = df_new['average_usage_cpu'] / df_new['resource_request_cpu']\n",
    "        # 处理无穷值\n",
    "        df_new['cpu_utilization_ratio'] = df_new['cpu_utilization_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "        # 上限为1（100%利用率）\n",
    "        df_new['cpu_utilization_ratio'] = df_new['cpu_utilization_ratio'].clip(upper=1.0)\n",
    "    \n",
    "    if 'resource_request_memory' in df.columns and 'average_usage_memory' in df.columns:\n",
    "        # 内存使用率 = 实际使用 / 请求资源\n",
    "        df_new['memory_utilization_ratio'] = df_new['average_usage_memory'] / df_new['resource_request_memory']\n",
    "        # 处理无穷值\n",
    "        df_new['memory_utilization_ratio'] = df_new['memory_utilization_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "        # 上限为1（100%利用率）\n",
    "        df_new['memory_utilization_ratio'] = df_new['memory_utilization_ratio'].clip(upper=1.0)\n",
    "    \n",
    "    # 资源效率比率（如果CPU和内存指标都存在）\n",
    "    if 'cpu_utilization_ratio' in df_new.columns and 'memory_utilization_ratio' in df_new.columns:\n",
    "        # 资源平衡指标（接近1表示CPU和内存使用平衡）\n",
    "        df_new['resource_balance_ratio'] = df_new['cpu_utilization_ratio'] / df_new['memory_utilization_ratio']\n",
    "        # 处理无穷值\n",
    "        df_new['resource_balance_ratio'] = df_new['resource_balance_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# 应用资源使用率特征创建\n",
    "df = create_utilization_features(df)\n",
    "print(\"已创建资源使用率特征\")\n",
    "\n",
    "# 显示新增特征\n",
    "utilization_cols = ['cpu_utilization_ratio', 'memory_utilization_ratio', 'resource_balance_ratio']\n",
    "utilization_cols = [col for col in utilization_cols if col in df.columns]\n",
    "if utilization_cols:\n",
    "    display(df[utilization_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 任务特性特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理任务特性特征\n",
    "task_features = ['priority', 'scheduling_class', 'collection_type', 'vertical_scaling', 'instance_index', 'failed']\n",
    "task_features = [col for col in task_features if col in df.columns]\n",
    "\n",
    "if task_features:\n",
    "    print(f\"发现任务特性特征: {task_features}\")\n",
    "    \n",
    "    # 对分类特征进行独热编码\n",
    "    categorical_features = []\n",
    "    for col in task_features:\n",
    "        if df[col].dtype == 'object' or df[col].nunique() < 10:  # 分类特征判断条件\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    if categorical_features:\n",
    "        print(f\"将进行独热编码的分类特征: {categorical_features}\")\n",
    "        df = pd.get_dummies(df, columns=categorical_features, prefix=categorical_features)\n",
    "        \n",
    "    # 显示处理后的前几行\n",
    "    new_cols = [col for col in df.columns if any(col.startswith(f\"{feat}_\") for feat in categorical_features)]\n",
    "    if new_cols:\n",
    "        display(df[new_cols[:5]].head())  # 只显示前5个新列\n",
    "else:\n",
    "    print(\"未找到任务特性特征\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 内存和CPU效率指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理CPU和内存效率指标\n",
    "efficiency_features = ['cycles_per_instruction', 'memory_accesses_per_instruction',\n",
    "                       'assigned_memory', 'page_cache_memory']\n",
    "efficiency_features = [col for col in efficiency_features if col in df.columns]\n",
    "\n",
    "if efficiency_features:\n",
    "    print(f\"发现效率指标特征: {efficiency_features}\")\n",
    "    \n",
    "    # 检查这些特征的缺失情况\n",
    "    missing = df[efficiency_features].isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    \n",
    "    for col, miss, pct in zip(efficiency_features, missing, missing_pct):\n",
    "        print(f\"{col}: {miss} 缺失值 ({pct:.2f}%)\")\n",
    "        \n",
    "        # 如果缺失值不太多，使用中位数填充\n",
    "        if pct < 50:\n",
    "            median_val = df[col].median()\n",
    "            df[col] = df[col].fillna(median_val)\n",
    "            print(f\"  - 使用中位数 {median_val:.6f} 填充缺失值\")\n",
    "            \n",
    "    # 显示处理后的效率指标\n",
    "    display(df[efficiency_features].describe())\n",
    "    \n",
    "    # 创建新的复合效率指标\n",
    "    if 'cycles_per_instruction' in df.columns and 'memory_accesses_per_instruction' in df.columns:\n",
    "        # 计算计算密集型指标 (高CPI, 低MAI意味着计算密集)\n",
    "        df['compute_intensity'] = df['cycles_per_instruction'] / (df['memory_accesses_per_instruction'] + 0.001)\n",
    "        print(\"已创建计算密集型指标\")\n",
    "        \n",
    "    if 'assigned_memory' in df.columns and 'page_cache_memory' in df.columns:\n",
    "        # 计算缓存使用比例\n",
    "        df['cache_ratio'] = df['page_cache_memory'] / (df['assigned_memory'] + 0.0001)\n",
    "        print(\"已创建缓存使用比例\")\n",
    "        \n",
    "    # 显示新创建的指标\n",
    "    new_metrics = ['compute_intensity', 'cache_ratio']\n",
    "    new_metrics = [col for col in new_metrics if col in df.columns]\n",
    "    if new_metrics:\n",
    "        display(df[new_metrics].describe())\n",
    "else:\n",
    "    print(\"未找到效率指标特征\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备模型训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df, target_vars):\n",
    "    \"\"\"准备模型训练数据\"\"\"\n",
    "    print(\"\\n准备模型训练数据...\")\n",
    "    \n",
    "    # 处理缺失值\n",
    "    print(\"处理缺失值...\")\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                # 数值型列用中位数填充\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "            else:\n",
    "                # 非数值型列用众数填充\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # 删除无用列\n",
    "    cols_to_drop = []\n",
    "    \n",
    "    # 删除高基数ID列\n",
    "    id_cols = [col for col in df.columns if 'id' in col.lower() or 'name' in col.lower() or 'user' in col.lower()]\n",
    "    cols_to_drop.extend(id_cols)\n",
    "    \n",
    "    # 删除原始时间戳列（保留转换后的dt列）\n",
    "    timestamp_cols = [col for col in df.columns if ('time' in col.lower() and 'dt' not in col.lower())]\n",
    "    cols_to_drop.extend(timestamp_cols)\n",
    "    \n",
    "    # 排除目标变量\n",
    "    cols_to_drop = [col for col in cols_to_drop if col not in target_vars]\n",
    "    \n",
    "    # 删除全是NaN的列\n",
    "    null_cols = df.columns[df.isnull().all()].tolist()\n",
    "    cols_to_drop.extend(null_cols)\n",
    "    \n",
    "    # 删除列\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
    "    print(f\"删除了 {len(cols_to_drop)} 列\")\n",
    "    \n",
    "    # 将分类变量转换为数值\n",
    "    object_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in object_cols:\n",
    "        if col not in target_vars:  # 不转换目标变量\n",
    "            # 对分类变量进行标签编码\n",
    "            df[col] = pd.factorize(df[col])[0]\n",
    "    \n",
    "    print(\"数据准备完成\")\n",
    "    return df\n",
    "\n",
    "df = prepare_data_for_modeling(df, target_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 为每个目标变量构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 可视化预测结果\n",
    "def visualize_predictions_separate(y_true, predictions_dict, title_prefix=\"Prediction Comparison\"):\n",
    "    \"\"\"Create a separate prediction vs true value comparison chart for each model\"\"\"\n",
    "    \n",
    "    # First, create an overview chart containing all models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(len(y_true)), y_true, 'k-', label='True Value')\n",
    "    \n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        plt.plot(range(len(preds)), preds, '--', label=f'{model_name} Prediction')\n",
    "    \n",
    "    plt.title(f\"{title_prefix} - Overview\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Target Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title_prefix.replace(' ', '_')}_overview.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Then, create a separate chart for each model\n",
    "    for model_name, preds in predictions_dict.items():\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(len(y_true)), y_true, 'k-', label='True Value')\n",
    "        plt.plot(range(len(preds)), preds, 'r--', label=f'{model_name} Prediction')\n",
    "        \n",
    "        plt.title(f\"{title_prefix} - {model_name}\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Target Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{title_prefix.replace(' ', '_')}_{model_name.replace(' ', '_')}.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(f\"Created {len(predictions_dict) + 1} prediction comparison charts\")\n",
    "\n",
    "# 4. 模型构建与评估\n",
    "def evaluate_models(df, target_var, test_size=0.2, random_state=42):\n",
    "    \"\"\"Build and evaluate multiple prediction models\"\"\"\n",
    "    print(f\"\\nEvaluating prediction models for {target_var}...\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    y = df[target_var]\n",
    "    X = df.drop(columns=[col for col in df.columns if col in [target_var] or col.startswith('time_')])\n",
    "    \n",
    "    print(f\"Feature count: {X.shape[1]}\")\n",
    "    print(f\"Sample count: {X.shape[0]}\")\n",
    "    \n",
    "    # 创建训练集和测试集 (时间序列分割)\n",
    "    # 为确保我们不用未来数据预测过去，使用最后test_size比例的数据作为测试集\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    # 特征标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 模型结果存储\n",
    "    model_results = []\n",
    "    \n",
    "    # 1. ARIMA model\n",
    "    try:\n",
    "        print(\"\\nTraining ARIMA model...\")\n",
    "        # 简化ARIMA，仅使用目标变量的时间序列\n",
    "        # 对于复杂变量，可能需要使用SARIMAX\n",
    "        model = ARIMA(y_train, order=(5,1,0))\n",
    "        arima_model = model.fit()\n",
    "        \n",
    "        # Predict\n",
    "        arima_preds = arima_model.forecast(steps=len(y_test))\n",
    "        \n",
    "        # Evaluate\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, arima_preds))\n",
    "        mae = mean_absolute_error(y_test, arima_preds)\n",
    "        r2 = r2_score(y_test, arima_preds)\n",
    "        \n",
    "        print(f\"ARIMA - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "        model_results.append({\"model\": \"ARIMA\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA model training failed: {e}\")\n",
    "    \n",
    "    # 2. Random Forest\n",
    "    print(\"\\nTraining Random Forest...\")\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=random_state)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    rf_preds = rf.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, rf_preds))\n",
    "    mae = mean_absolute_error(y_test, rf_preds)\n",
    "    r2 = r2_score(y_test, rf_preds)\n",
    "    \n",
    "    print(f\"Random Forest - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "    model_results.append({\"model\": \"Random Forest\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 feature importance:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # 3. XGBoost\n",
    "    print(\"\\nTraining XGBoost...\")\n",
    "    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=random_state)\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    xgb_preds = xgb_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, xgb_preds))\n",
    "    mae = mean_absolute_error(y_test, xgb_preds)\n",
    "    r2 = r2_score(y_test, xgb_preds)\n",
    "    \n",
    "    print(f\"XGBoost - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "    model_results.append({\"model\": \"XGBoost\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "    \n",
    "    # 4. LSTM (if TensorFlow is available)\n",
    "    if tf_available and len(X_train) > 50:  # Ensure there's enough data\n",
    "        try:\n",
    "            print(\"\\nTraining LSTM model...\")\n",
    "            \n",
    "            # 准备LSTM输入 (样本, 时间步, 特征)\n",
    "            # 这里我们使用最简单的方式：每个样本的前LOOKBACK个时间步作为输入\n",
    "            LOOKBACK = 5\n",
    "            \n",
    "            def create_sequences(X, y, time_steps=LOOKBACK):\n",
    "                X_seq, y_seq = [], []\n",
    "                for i in range(len(X) - time_steps):\n",
    "                    X_seq.append(X[i:i + time_steps])\n",
    "                    y_seq.append(y[i + time_steps])\n",
    "                return np.array(X_seq), np.array(y_seq)\n",
    "            \n",
    "            # Create sequences\n",
    "            X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values)\n",
    "            \n",
    "            # Build LSTM model\n",
    "            model = Sequential([\n",
    "                LSTM(50, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2]), return_sequences=True),\n",
    "                Dropout(0.2),\n",
    "                LSTM(50, activation='relu'),\n",
    "                Dropout(0.2),\n",
    "                Dense(1)\n",
    "            ])\n",
    "            \n",
    "            model.compile(optimizer='adam', loss='mse')\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            \n",
    "            # Train\n",
    "            history = model.fit(\n",
    "                X_train_seq, y_train_seq,\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # 为测试集创建序列\n",
    "            # 注意：我们需要包括训练集的最后几个样本来预测测试集的第一个样本\n",
    "            X_full = np.vstack((X_train_scaled[-LOOKBACK:], X_test_scaled))\n",
    "            X_test_seq = []\n",
    "            for i in range(len(X_test)):\n",
    "                X_test_seq.append(X_full[i:i + LOOKBACK])\n",
    "            X_test_seq = np.array(X_test_seq)\n",
    "            \n",
    "            # Predict\n",
    "            lstm_preds = model.predict(X_test_seq).flatten()\n",
    "            \n",
    "            # Evaluate\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, lstm_preds))\n",
    "            mae = mean_absolute_error(y_test, lstm_preds)\n",
    "            r2 = r2_score(y_test, lstm_preds)\n",
    "            \n",
    "            print(f\"LSTM - RMSE: {rmse:.6f}, MAE: {mae:.6f}, R²: {r2:.6f}\")\n",
    "            model_results.append({\"model\": \"LSTM\", \"rmse\": rmse, \"mae\": mae, \"r2\": r2})\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"LSTM model training failed: {e}\")\n",
    "    \n",
    "    # Summarize results\n",
    "    results_df = pd.DataFrame(model_results)\n",
    "    results_df = results_df.sort_values('rmse')\n",
    "    \n",
    "    print(\"\\nModel performance summary:\")\n",
    "    print(results_df)\n",
    "\n",
    "    # 可视化预测结果\n",
    "    visualize_predictions_separate(\n",
    "        y_true=y_test, \n",
    "        predictions_dict=predictions_dict,\n",
    "        title_prefix=f\"{target_var} Prediction Comparison\"\n",
    "    )\n",
    "    \n",
    "    # Return the best model and evaluation results\n",
    "    return results_df, feature_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_var in target_vars:\n",
    "    # 过滤掉含有NaN的行\n",
    "    df_clean = df.dropna(subset=[target_var])\n",
    "    \n",
    "    # 过滤掉其他目标变量的滞后特征\n",
    "    other_targets = [t for t in target_vars if t != target_var]\n",
    "    cols_to_drop = []\n",
    "    for other_target in other_targets:\n",
    "        cols_to_drop.extend([col for col in df_clean.columns if col.startswith(f\"{other_target}_lag_\")])\n",
    "        cols_to_drop.extend([col for col in df_clean.columns if col.startswith(f\"{other_target}_rolling_\")])\n",
    "    \n",
    "    df_model = df_clean.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # 构建和评估模型\n",
    "    results_df, feature_importance = evaluate_models(df_model, target_var)\n",
    "    \n",
    "    # 保存结果\n",
    "    results_df.to_csv(f\"model_results_{target_var}.csv\", index=False)\n",
    "    feature_importance.to_csv(f\"feature_importance_{target_var}.csv\", index=False)\n",
    "\n",
    "print(\"建模完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
